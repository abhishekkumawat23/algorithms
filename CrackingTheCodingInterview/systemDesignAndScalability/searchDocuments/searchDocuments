Question:
Given a list of millions of documents, how would you find all documents that contain a list of words?
The words can appear in any order, but they must be complete words.
That is "book" doesn't match "bookkeeper".

Answer: This is `part of system` question.
Step 1: Make believe
Temporary assumptions:
1. Number of documents are less but greater than length of words list.
2. All documents can fit on one machine.
3. Documents are searched only once.

Time: O(n*m') to created inverted index mapping + O(m) to find word with min documents + O(n*m) to intersect documents of each word.
Space: O(n+m) to store map + O(n) to store intersected documents. 
Here n is number of documents
m is length of words list
m' is number of words in each document. This will be way greater than m.
searchDocuments(words, documents):
  wordDocumentMap = createMap(words) // Add empty document set as value
  // Create inverted index mapping from words to documents.
  for document in documents
    for word in document
      if wordDocumentMap.containsKey(word)
        if !wordDocumentMap.get(word).containsKey(document)
          wordDocumentMap.get(word).add(document)
  // Find word where number of documents are least. We will iterate on those documents.
  wordWithMinDocuments = findWordWithMinDocuments(worDocumentMap)
  // Take intersection of wordDocumentMap values
  for document in wordDocumentMap.get(wordWithMinDocuments)
    intersectionFailed = false
    for entry in wordDocumentMap
      if !entry.getValue().containsKey(document)
        intersectionFailed = true
    if !intersectionFailed
      results.add(document)
  return results
  
Step 2: Get real
Issues in getting real:
1. Millions documents can't be stored on one machine's memory
2. Inverted index map might not be stored on one machine for millions of documents.
3. Processing time will be huge create inverted index map and then find intersection of documents using that inverted index map.
4. Documents will be searched very often with different set of word list
5. Is there any other of efficiency as well?

Step 3:
Solutions:
1. Millions documents can't be stored on one machine's memory
   a. Store documents in disk. Load only those documents in memory which we are reading.
   b. Store documents across various machines. We can store documents name starting from A on one machine, B on second machine etc.
2. Inverted index map might not be stored on one machine for millions of documents.
   a. As documents are now divided on machines, we can create and store inverted index map for those documents in that machine itself.
3. Processing time will be huge create inverted index map and then find intersection of documents using that inverted index map.
   a. Due to dividing documents across various machines, we now need to do less processing.
   b. Still inverted index map is very heavy as it takes O(n*m') where m' is number of words in each document and n is no of documents.
   b. We can create inverted index map in parallel. Within a machine, few documents will be processed by one task, few other by another task etc.
   c. Still inverted index map is bad as it requires documents to be loaded in memory and then each word of each document.
   d. We need for all parallel tasks of inverted index map to complete as we need inverted index map completely ready before starting intersection.
   e. Once inverted index map is ready, we can find intersection of documents using parallel tasks.
   f. Few documents of a word in inverted index map will be intersected by one task, few others by another task and so on.
4. Documents will be searched very often with different set of word list
   a. This is a real scenario. People will search with different word list every time.
   b. In this case we can do some pre-processing.
   c. To add a new document, load balancer will check the lookup table that in which machine to send this new document based on title.
   d. Document will get added in documents present in that machine.
   e. As soon as a document is inserted in documents, we will create the inverted index map of it. The inverted index map will contain all words.
   d. Now whenever any user searches with words list, we will simply find the intersection of documents of those words from inverted index map. And this will happen in parallel tasks.
   e. Thus we have increased efficiency a lot as inverted index mapping is no longer present as its pre-processed.
   f. Inverted index step was high time consuming as it was O(n*m') and also it involved loading document in memory.
   g. Intersection is light weight as its O(n*m) where n will be in rare scenario when the word is in all documents. Also, there is no document load from memory.
5. Is there any other way of efficiency as well?
   a. Currently we divided the documents across machines. We can alternatively divide words list across machines.
   b. In current pre-processing of inverted index map, each machine has a inverted index map with all words containing documents only which are present in that machine.
   d. We can do this pre-processing in another way. Whenever a document is added, we will read the document and words starting from A will go to one machine inverted index map, words from B to another machine's inverted index map.
   e. This way every machine will have a inverted index map which has words starting from some particular alphabet.
   f. Now whenever user passes a words list, we will sort it. Now all words starting from A will be passed to machine1 etc.
   g. Every machine will do the intersection of words passed to it.
   h. The wrapper machines of all will intersect the documents received from all machines.
   i. This way is little complex than earlier one.